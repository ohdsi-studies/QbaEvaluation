@ARTICLE{Lash2014-jd,
  title    = "Good practices for quantitative bias analysis",
  author   = "Lash, Timothy L and Fox, Matthew P and MacLehose, Richard F and
              Maldonado, George and McCandless, Lawrence C and Greenland,
              Sander",
  abstract = "Quantitative bias analysis serves several objectives in
              epidemiological research. First, it provides a quantitative
              estimate of the direction, magnitude and uncertainty arising from
              systematic errors. Second, the acts of identifying sources of
              systematic error, writing down models to quantify them, assigning
              values to the bias parameters and interpreting the results combat
              the human tendency towards overconfidence in research results,
              syntheses and critiques and the inferences that rest upon them.
              Finally, by suggesting aspects that dominate uncertainty in a
              particular research result or topic area, bias analysis can guide
              efficient allocation of sparse research resources. The
              fundamental methods of bias analyses have been known for decades,
              and there have been calls for more widespread use for nearly as
              long. There was a time when some believed that bias analyses were
              rarely undertaken because the methods were not widely known and
              because automated computing tools were not readily available to
              implement the methods. These shortcomings have been largely
              resolved. We must, therefore, contemplate other barriers to
              implementation. One possibility is that practitioners avoid the
              analyses because they lack confidence in the practice of bias
              analysis. The purpose of this paper is therefore to describe what
              we view as good practices for applying quantitative bias analysis
              to epidemiological data, directed towards those familiar with the
              methods. We focus on answering questions often posed to those of
              us who advocate incorporation of bias analysis methods into
              teaching and research. These include the following. When is bias
              analysis practical and productive? How does one select the biases
              that ought to be addressed? How does one select a method to model
              biases? How does one assign values to the parameters of a bias
              model? How does one present and interpret a bias analysis?. We
              hope that our guide to good practices for conducting and
              presenting bias analyses will encourage more widespread use of
              bias analysis to estimate the potential magnitude and direction
              of biases, as well as the uncertainty in estimates potentially
              influenced by the biases.",
  journal  = "Int. J. Epidemiol.",
  volume   =  43,
  number   =  6,
  pages    = "1969--1985",
  month    =  dec,
  year     =  2014,
  keywords = "Epidemiological biases; analysis; best practice",
  language = "en"
}




@ARTICLE{Austin2011-we,
  title    = "An Introduction to Propensity Score Methods for Reducing the
              Effects of Confounding in Observational Studies",
  author   = "Austin, Peter C",
  abstract = "The propensity score is the probability of treatment assignment
              conditional on observed baseline characteristics. The propensity
              score allows one to design and analyze an observational
              (nonrandomized) study so that it mimics some of the particular
              characteristics of a randomized controlled trial. In particular,
              the propensity score is a balancing score: conditional on the
              propensity score, the distribution of observed baseline
              covariates will be similar between treated and untreated
              subjects. I describe 4 different propensity score methods:
              matching on the propensity score, stratification on the
              propensity score, inverse probability of treatment weighting
              using the propensity score, and covariate adjustment using the
              propensity score. I describe balance diagnostics for examining
              whether the propensity score model has been adequately specified.
              Furthermore, I discuss differences between regression-based
              methods and propensity score-based methods for the analysis of
              observational data. I describe different causal average treatment
              effects and their relationship with propensity score analyses.",
  journal  = "Multivariate Behav. Res.",
  volume   =  46,
  number   =  3,
  pages    = "399--424",
  month    =  may,
  year     =  2011,
  language = "en"
}

@ARTICLE{Rosenbaum1983-hi,
  title     = "The central role of the propensity score in observational
               studies for causal effects",
  author    = "Rosenbaum, Paul R and Rubin, Donald B",
  abstract  = "Abstract. The propensity score is the conditional probability of
               assignment to a particular treatment given a vector of observed
               covariates. Both large and smal",
  journal   = "Biometrika",
  publisher = "Oxford Academic",
  volume    =  70,
  number    =  1,
  pages     = "41--55",
  month     =  apr,
  year      =  1983,
  language  = "en"
}

@ARTICLE{Stuart2015-gg,
  title    = "Assessing the generalizability of randomized trial results to
              target populations",
  author   = "Stuart, Elizabeth A and Bradshaw, Catherine P and Leaf, Philip J",
  abstract = "Recent years have seen increasing interest in and attention to
              evidence-based practices, where the ``evidence'' generally comes
              from well-conducted randomized trials. However, while those
              trials yield accurate estimates of the effect of the intervention
              for the participants in the trial (known as ``internal
              validity''), they do not always yield relevant information about
              the effects in a particular target population (known as
              ``external validity''). This may be due to a lack of
              specification of a target population when designing the trial,
              difficulties recruiting a sample that is representative of a
              prespecified target population, or to interest in considering a
              target population somewhat different from the population directly
              targeted by the trial. This paper first provides an overview of
              existing design and analysis methods for assessing and enhancing
              the ability of a randomized trial to estimate treatment effects
              in a target population. It then provides a case study using one
              particular method, which weights the subjects in a randomized
              trial to match the population on a set of observed
              characteristics. The case study uses data from a randomized trial
              of school-wide positive behavioral interventions and supports
              (PBIS); our interest is in generalizing the results to the state
              of Maryland. In the case of PBIS, after weighting, estimated
              effects in the target population were similar to those observed
              in the randomized trial. The paper illustrates that statistical
              methods can be used to assess and enhance the external validity
              of randomized trials, making the results more applicable to
              policy and clinical questions. However, there are also many open
              research questions; future research should focus on questions of
              treatment effect heterogeneity and further developing these
              methods for enhancing external validity. Researchers should think
              carefully about the external validity of randomized trials and be
              cautious about extrapolating results to specific populations
              unless they are confident of the similarity between the trial
              sample and that target population.",
  journal  = "Prev. Sci.",
  volume   =  16,
  number   =  3,
  pages    = "475--485",
  month    =  apr,
  year     =  2015,
  language = "en"
}

@ARTICLE{Sherman2016-yn,
  title    = "{Real-World} Evidence - What Is It and What Can It Tell Us?",
  author   = "Sherman, Rachel E and Anderson, Steven A and Dal Pan, Gerald J
              and Gray, Gerry W and Gross, Thomas and Hunter, Nina L and
              LaVange, Lisa and Marinac-Dabic, Danica and Marks, Peter W and
              Robb, Melissa A and Shuren, Jeffrey and Temple, Robert and
              Woodcock, Janet and Yue, Lilly Q and Califf, Robert M",
  journal  = "N. Engl. J. Med.",
  volume   =  375,
  number   =  23,
  pages    = "2293--2297",
  month    =  dec,
  year     =  2016,
  language = "en"
}

@BOOK{Lash2009-ch,
  title     = "Applying Quantitative Bias Analysis to Epidemiologic Data",
  author    = "Lash, Timothy L and Fox, Matthew P and Fink, Aliza K",
  abstract  = "Bias analysis quantifies the influence of systematic error on an
               epidemiology study's estimate of association. The fundamental
               methods of bias analysis in epi- miology have been well
               described for decades, yet are seldom applied in published
               presentations of epidemiologic research. More recent advances in
               bias analysis, such as probabilistic bias analysis, appear even
               more rarely. We suspect that there are both supply-side and
               demand-side explanations for the scarcity of bias analysis. On
               the demand side, journal reviewers and editors seldom request
               that authors address systematic error aside from listing them as
               limitations of their particular study. This listing is often
               accompanied by explanations for why the limitations should not
               pose much concern. On the supply side, methods for bias analysis
               receive little attention in most epidemiology curriculums, are
               often scattered throughout textbooks or absent from them
               altogether, and cannot be implemented easily using standard
               statistical computing software. Our objective in this text is to
               reduce these supply-side barriers, with the hope that demand for
               quantitative bias analysis will follow.",
  publisher = "Springer New York",
  month     =  may,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Lash2014-jd,
  title    = "Good practices for quantitative bias analysis",
  author   = "Lash, Timothy L and Fox, Matthew P and MacLehose, Richard F and
              Maldonado, George and McCandless, Lawrence C and Greenland,
              Sander",
  abstract = "Quantitative bias analysis serves several objectives in
              epidemiological research. First, it provides a quantitative
              estimate of the direction, magnitude and uncertainty arising from
              systematic errors. Second, the acts of identifying sources of
              systematic error, writing down models to quantify them, assigning
              values to the bias parameters and interpreting the results combat
              the human tendency towards overconfidence in research results,
              syntheses and critiques and the inferences that rest upon them.
              Finally, by suggesting aspects that dominate uncertainty in a
              particular research result or topic area, bias analysis can guide
              efficient allocation of sparse research resources. The
              fundamental methods of bias analyses have been known for decades,
              and there have been calls for more widespread use for nearly as
              long. There was a time when some believed that bias analyses were
              rarely undertaken because the methods were not widely known and
              because automated computing tools were not readily available to
              implement the methods. These shortcomings have been largely
              resolved. We must, therefore, contemplate other barriers to
              implementation. One possibility is that practitioners avoid the
              analyses because they lack confidence in the practice of bias
              analysis. The purpose of this paper is therefore to describe what
              we view as good practices for applying quantitative bias analysis
              to epidemiological data, directed towards those familiar with the
              methods. We focus on answering questions often posed to those of
              us who advocate incorporation of bias analysis methods into
              teaching and research. These include the following. When is bias
              analysis practical and productive? How does one select the biases
              that ought to be addressed? How does one select a method to model
              biases? How does one assign values to the parameters of a bias
              model? How does one present and interpret a bias analysis?. We
              hope that our guide to good practices for conducting and
              presenting bias analyses will encourage more widespread use of
              bias analysis to estimate the potential magnitude and direction
              of biases, as well as the uncertainty in estimates potentially
              influenced by the biases.",
  journal  = "Int. J. Epidemiol.",
  volume   =  43,
  number   =  6,
  pages    = "1969--1985",
  month    =  dec,
  year     =  2014,
  keywords = "Epidemiological biases; analysis; best practice",
  language = "en"
}

@ARTICLE{Swerdel2019-fl,
  title    = "{PheValuator}: Development and evaluation of a phenotype
              algorithm evaluator",
  author   = "Swerdel, Joel N and Hripcsak, George and Ryan, Patrick B",
  abstract = "BACKGROUND: The primary approach for defining disease in
              observational healthcare databases is to construct phenotype
              algorithms (PAs), rule-based heuristics predicated on the
              presence, absence, and temporal logic of clinical observations.
              However, a complete evaluation of PAs, i.e., determining
              sensitivity, specificity, and positive predictive value (PPV), is
              rarely performed. In this study, we propose a tool (PheValuator)
              to efficiently estimate a complete PA evaluation. METHODS: We
              used 4 administrative claims datasets: OptumInsight's
              de-identified ClinformaticsT Datamart (Eden Prairie,MN); IBM
              MarketScan Multi-State Medicaid); IBM MarketScan Medicare
              Supplemental Beneficiaries; and IBM MarketScan Commercial Claims
              and Encounters from 2000 to 2017. Using PheValuator involves (1)
              creating a diagnostic predictive model for the phenotype, (2)
              applying the model to a large set of randomly selected subjects,
              and (3) comparing each subject's predicted probability for the
              phenotype to inclusion/exclusion in PAs. We used the predictions
              as a 'probabilistic gold standard' measure to classify
              positive/negative cases. We examined 4 phenotypes: myocardial
              infarction, cerebral infarction, chronic kidney disease, and
              atrial fibrillation. We examined several PAs for each phenotype
              including 1-time (1X) occurrence of the diagnosis code in the
              subject's record and 1-time occurrence of the diagnosis in an
              inpatient setting with the diagnosis code as the primary reason
              for admission (1X-IP-1stPos). RESULTS: Across phenotypes, the 1X
              PA showed the highest sensitivity/lowest PPV among all PAs.
              1X-IP-1stPos yielded the highest PPV/lowest sensitivity.
              Specificity was very high across algorithms. We found similar
              results between algorithms across datasets. CONCLUSION:
              PheValuator appears to show promise as a tool to estimate PA
              performance characteristics.",
  journal  = "J. Biomed. Inform.",
  volume   =  97,
  pages    = "103258",
  month    =  sep,
  year     =  2019,
  keywords = "Diagnostic predictive modeling; Phenotype algorithms; Validation",
  language = "en"
}

@ARTICLE{Swerdel2022-pi,
  title    = "{PheValuator} 2.0: Methodological improvements for the
              {PheValuator} approach to semi-automated phenotype algorithm
              evaluation",
  author   = "Swerdel, Joel N and Schuemie, Martijn and Murray, Gayle and Ryan,
              Patrick B",
  abstract = "PURPOSE: Phenotype algorithms are central to performing analyses
              using observational data. These algorithms translate the clinical
              idea of a health condition into an executable set of rules
              allowing for queries of data elements from a database.
              PheValuator, a software package in the Observational Health Data
              Sciences and Informatics (OHDSI) tool stack, provides a method to
              assess the performance characteristics of these algorithms,
              namely, sensitivity, specificity, and positive and negative
              predictive value. It uses machine learning to develop predictive
              models for determining a probabilistic gold standard of subjects
              for assessment of cases and non-cases of health conditions.
              PheValuator was developed to complement or even replace the
              traditional approach of algorithm validation, i.e., by expert
              assessment of subject records through chart review. Results in
              our first PheValuator paper suggest a systematic underestimation
              of the PPV compared to previous results using chart review. In
              this paper we evaluate modifications made to the method designed
              to improve its performance. METHODS: The major changes to
              PheValuator included allowing all diagnostic conditions, clinical
              observations, drug prescriptions, and laboratory measurements to
              be included as predictors within the modeling process whereas in
              the prior version there were significant restrictions on the
              included predictors. We also have allowed for the inclusion of
              the temporal relationships of the predictors in the model. To
              evaluate the performance of the new method, we compared the
              results from the new and original methods against results found
              from the literature using traditional validation of algorithms
              for 19 phenotypes. We performed these tests using data from five
              commercial databases. RESULTS: In the assessment aggregating all
              phenotype algorithms, the median difference between the
              PheValuator estimate and the gold standard estimate for PPV was
              reduced from -21 (IQR -34, -3) in Version 1.0 to 4 (IQR -3, 15)
              using Version 2.0. We found a median difference in specificity of
              3 (IQR 1, 4.25) for Version 1.0 and 3 (IQR 1, 4) for Version 2.0.
              The median difference between the two versions of PheValuator and
              the gold standard for estimates of sensitivity was reduced from
              -39 (-51, -20) to -16 (-34, -6). CONCLUSION: PheValuator 2.0
              produces estimates for the performance characteristics for
              phenotype algorithms that are significantly closer to estimates
              from traditional validation through chart review compared to
              version 1.0. With this tool in researcher's toolkits, methods,
              such as quantitative bias analysis, may now be used to improve
              the reliability and reproducibility of research studies using
              observational data.",
  journal  = "J. Biomed. Inform.",
  volume   =  135,
  pages    = "104177",
  month    =  nov,
  year     =  2022,
  keywords = "Phenotype algorithms; Positive predictive value; Sensitivity;
              Specificity",
  language = "en"
}




